break the following implementation up into 2 tasks

# Implementation Plan — GCS-backed Images for Multimodal Chat

## 0) Goals & Constraints

* Store **both** user-uploaded and model-generated images in **private** GCS.
* **No local storage** beyond temp streams; **no download/serve endpoints**.
* Chat history stores **URLs only** (signed GCS links), not image blobs.
* Keep existing SQLite message/attachment records; extend schema as needed.
* Respect existing env:

  ```
  GCS_BUCKET_NAME=openrouter-chat
  GCP_PROJECT_ID=pihome123
  GOOGLE_APPLICATION_CREDENTIALS=credentials/googlecloud/sa.json
  GCS_PUBLIC_URL_TEMPLATE=...  # ❌ not used anymore for downloads
  ATTACHMENTS_MAX_SIZE_BYTES=10485760
  ATTACHMENTS_RETENTION_DAYS=7
  ```

---

## 1) Dependencies

**pyproject.toml / requirements.txt**

```toml
google-cloud-storage>=2.17.0
python-dotenv>=1.0.1
```

---

## 2) Settings

**app/settings.py** (or wherever your config lives)

```python
from pydantic import BaseSettings, AnyUrl
from datetime import timedelta

class Settings(BaseSettings):
    GCS_BUCKET_NAME: str = "openrouter-chat"
    GCP_PROJECT_ID: str = "pihome123"
    GOOGLE_APPLICATION_CREDENTIALS: str = "credentials/googlecloud/sa.json"

    ATTACHMENTS_MAX_SIZE_BYTES: int = 10 * 1024 * 1024
    ATTACHMENTS_RETENTION_DAYS: int = 7

    # Deprecated: we no longer proxy downloads
    GCS_PUBLIC_URL_TEMPLATE: str | None = None

    @property
    def attachment_signed_url_ttl(self) -> timedelta:
        return timedelta(days=self.ATTACHMENTS_RETENTION_DAYS)

settings = Settings()
```

---

## 3) GCS Utility

**app/services/gcs.py**

```python
from __future__ import annotations
from google.cloud import storage
from datetime import datetime
from typing import BinaryIO
import io

from app.settings import settings

_client: storage.Client | None = None
_bucket: storage.Bucket | None = None

def get_client() -> storage.Client:
    global _client
    if _client is None:
        _client = storage.Client(project=settings.GCP_PROJECT_ID)
    return _client

def get_bucket() -> storage.Bucket:
    global _bucket
    if _bucket is None:
        _bucket = get_client().bucket(settings.GCS_BUCKET_NAME)
    return _bucket

def upload_bytes(blob_name: str, data: bytes, *, content_type: str) -> None:
    blob = get_bucket().blob(blob_name)
    blob.upload_from_string(data, content_type=content_type)

def upload_filelike(blob_name: str, file_like: BinaryIO, *, content_type: str) -> None:
    blob = get_bucket().blob(blob_name)
    blob.upload_from_file(file_like, content_type=content_type)

def delete_blob(blob_name: str) -> None:
    blob = get_bucket().blob(blob_name)
    blob.delete(if_generation_match=None)

def sign_get_url(blob_name: str, *, expires_delta) -> str:
    blob = get_bucket().blob(blob_name)
    return blob.generate_signed_url(
        version="v4",
        expiration=expires_delta,
        method="GET",
    )
```

---

## 4) DB Schema Changes (SQLite)

We’ll stop using local `storage_path` and keep a canonical **`gcs_blob`** name; we’ll **store a currently-valid signed URL** plus its **`signed_url_expires_at`**. (We will also refresh URLs dynamically when returning chat history.)

**migrations/xxxx_add_gcs_columns.sql**

```sql
ALTER TABLE attachments ADD COLUMN gcs_blob TEXT;
ALTER TABLE attachments ADD COLUMN signed_url TEXT;
ALTER TABLE attachments ADD COLUMN signed_url_expires_at TEXT; -- ISO8601
-- (optional) For backwards-compat: keep display_url/delivery_url but we will set them to the signed_url value.
```

> If you had a `storage_path` column, keep it for rollback, or migrate its value into `gcs_blob` format (`{session_id}/{attachment_id}__{safe_filename}`) and then ignore it in code.

---

## 5) Attachment ID & Blob Naming

**app/services/attachments_naming.py**

```python
import re
from pathlib import PurePosixPath

_filename_re = re.compile(r"[^A-Za-z0-9._-]+")

def make_blob_name(session_id: str, attachment_id: str, original_filename: str) -> str:
    safe = _filename_re.sub("_", original_filename).strip("._-") or "file"
    return str(PurePosixPath(session_id) / f"{attachment_id}__{safe}")
```

---

## 6) Attachment Service (Upload & Delete)

**app/services/attachments.py** — replace local-disk logic with GCS. This example shows a FastAPI `UploadFile` flow; use the same service for model-generated bytes.

```python
from __future__ import annotations
from datetime import datetime, timezone
from fastapi import UploadFile
from uuid import uuid4

from app.settings import settings
from app.services.gcs import upload_filelike, upload_bytes, sign_get_url, delete_blob
from app.services.attachments_naming import make_blob_name
from app.db.attachments_repo import AttachmentsRepo  # your existing repo

class AttachmentService:
    def __init__(self, repo: AttachmentsRepo):
        self.repo = repo

    async def save_user_upload(
        self, *, session_id: str, upload: UploadFile
    ) -> dict:
        # Validate size (streaming) and type
        mime = upload.content_type or "application/octet-stream"

        # If you already enforce types elsewhere, keep it. Otherwise validate here.
        # Size guard: if UploadFile doesn't expose length, manually sum chunks.
        size = 0
        chunks: list[bytes] = []
        while True:
            chunk = await upload.read(1024 * 1024)
            if not chunk:
                break
            size += len(chunk)
            if size > settings.ATTACHMENTS_MAX_SIZE_BYTES:
                raise ValueError("AttachmentTooLarge")
            chunks.append(chunk)
        data = b"".join(chunks)

        attachment_id = uuid4().hex
        blob_name = make_blob_name(session_id, attachment_id, upload.filename or "file.bin")

        # Upload to GCS
        upload_bytes(blob_name, data, content_type=mime)

        # Signed URL valid for retention period
        signed_url = sign_get_url(blob_name, expires_delta=settings.attachment_signed_url_ttl)
        expires_at = datetime.now(timezone.utc).replace(microsecond=0)
        signed_url_expires_at = expires_at + settings.attachment_signed_url_ttl

        record = await self.repo.insert_attachment(
            attachment_id=attachment_id,
            session_id=session_id,
            gcs_blob=blob_name,
            mime_type=mime,
            size_bytes=size,
            signed_url=signed_url,
            signed_url_expires_at=signed_url_expires_at.isoformat(),
            # For UI compatibility (if your types expect these):
            display_url=signed_url,
            delivery_url=signed_url,
        )
        return record

    async def save_model_image_bytes(
        self, *, session_id: str, data: bytes, mime_type: str, filename_hint: str = "image.png"
    ) -> dict:
        attachment_id = uuid4().hex
        blob_name = make_blob_name(session_id, attachment_id, filename_hint)
        upload_bytes(blob_name, data, content_type=mime_type)

        signed_url = sign_get_url(blob_name, expires_delta=settings.attachment_signed_url_ttl)
        now = datetime.now(timezone.utc).replace(microsecond=0)
        signed_url_expires_at = now + settings.attachment_signed_url_ttl

        return await self.repo.insert_attachment(
            attachment_id=attachment_id,
            session_id=session_id,
            gcs_blob=blob_name,
            mime_type=mime_type,
            size_bytes=len(data),
            signed_url=signed_url,
            signed_url_expires_at=signed_url_expires_at.isoformat(),
            display_url=signed_url,
            delivery_url=signed_url,
        )

    async def delete_attachment(self, attachment_id: str) -> None:
        rec = await self.repo.get_by_id(attachment_id)
        if rec and rec.get("gcs_blob"):
            try:
                delete_blob(rec["gcs_blob"])
            except Exception:
                # log & continue
                pass
        await self.repo.delete(attachment_id)
```

---

## 7) Repository Changes

**app/db/attachments_repo.py** — add fields for `gcs_blob`, `signed_url`, `signed_url_expires_at`. Keep `display_url` / `delivery_url` to satisfy existing API contracts, but populate them with `signed_url`.

```python
class AttachmentsRepo:
    async def insert_attachment(
        self, *, attachment_id, session_id, gcs_blob, mime_type,
        size_bytes, signed_url, signed_url_expires_at,
        display_url, delivery_url
    ) -> dict:
        # INSERT ... returning dict
        ...

    async def get_by_id(self, attachment_id: str) -> dict | None:
        ...

    async def delete(self, attachment_id: str) -> None:
        ...
```

---

## 8) Routes

### 8.1 POST /api/uploads  (keep)

* Keeps accepting `multipart/form-data`.
* Calls `AttachmentService.save_user_upload`.
* Returns your existing `AttachmentUploadResponse` shape, but with **GCS signed URLs**.

```python
@router.post("/api/uploads", response_model=AttachmentUploadResponse)
async def upload_attachment(...):
    rec = await svc.save_user_upload(session_id=session_id, upload=file)
    return AttachmentUploadResponse(
        attachment=AttachmentResource(
            id=rec["attachment_id"],
            displayUrl=rec["signed_url"],
            deliveryUrl=rec["signed_url"],
            mimeType=rec["mime_type"],
            sizeBytes=rec["size_bytes"],
            # include metadata you already return
        )
    )
```

### 8.2 ❌ Remove download/local endpoints

* Delete (or 410-Gone) anything like:

  * `GET /api/uploads/{attachment_id}/content`
  * Any local file path responders
* Delete local disk write paths (`ATTACHMENTS_DIR` usage) except temp.

> If you keep a DELETE endpoint for attachments (admin/UI cleanup), ensure it calls `AttachmentService.delete_attachment`.

---

## 9) Message Serialization — Refresh Expired URLs on Read

When the UI fetches **chat history** (e.g., `GET /api/sessions/{id}/messages`), make sure any attachment fragments have a **valid** URL. If `signed_url_expires_at <= now`, re-sign and patch the fields before returning.

**app/services/messages.py** (where you map DB → API shape)

```python
from datetime import datetime, timezone
from app.services.gcs import sign_get_url
from app.settings import settings
from app.db.attachments_repo import AttachmentsRepo

async def ensure_fresh_signed_url(att: dict, repo: AttachmentsRepo) -> dict:
    # att: row from 'attachments'
    expires_at = att.get("signed_url_expires_at")
    if expires_at:
        try:
            exp = datetime.fromisoformat(expires_at)
        except Exception:
            exp = datetime.now(timezone.utc)
    else:
        exp = datetime.now(timezone.utc)

    if exp <= datetime.now(timezone.utc):
        # Re-sign and persist
        new_url = sign_get_url(att["gcs_blob"], expires_delta=settings.attachment_signed_url_ttl)
        new_exp = datetime.now(timezone.utc) + settings.attachment_signed_url_ttl
        await repo.update_signed_url(att["attachment_id"], new_url, new_exp.isoformat())
        att["signed_url"] = new_url
        att["signed_url_expires_at"] = new_exp.isoformat()

    # For API response compatibility:
    att["display_url"] = att["signed_url"]
    att["delivery_url"] = att["signed_url"]
    return att
```

Call `ensure_fresh_signed_url` for every image attachment referenced in the messages you’re returning, then populate each `image_url.url` with `att["signed_url"]`.

---

## 10) Assistant (Model) Image Output

Where your assistant pipeline handles tool results or binary image outputs:

```python
# Suppose you have image bytes from a model/tool call:
rec = await attachment_service.save_model_image_bytes(
    session_id=session_id,
    data=image_bytes,
    mime_type="image/png",
    filename_hint="model.png",
)

# Build message content fragment
image_fragment = {
  "type": "image_url",
  "image_url": { "url": rec["signed_url"] },
  "metadata": {
    "attachment_id": rec["attachment_id"],
    "display_url": rec["signed_url"],
    "mime_type": rec["mime_type"],
    "size_bytes": rec["size_bytes"],
  }
}
# Insert into assistant message content as usual
```

No extra endpoints; the UI uses the signed GCS URL directly.

---

## 11) Cleanup / Retention

Create a small scheduled job (or reuse your housekeeping task) to delete expired attachments from **GCS** and DB.

**app/jobs/attachments_cleanup.py**

```python
from datetime import datetime, timezone
from app.db.attachments_repo import AttachmentsRepo
from app.services.gcs import delete_blob

async def cleanup_expired(repo: AttachmentsRepo):
    rows = await repo.find_expired_attachments(now=datetime.now(timezone.utc))
    for r in rows:
        try:
            if r.get("gcs_blob"):
                delete_blob(r["gcs_blob"])
        except Exception:
            # log and continue
            pass
        await repo.delete(r["attachment_id"])
```

Run this periodically (e.g., on app start with `asyncio.create_task`, or a cron in your process manager).

---

## 12) Remove Local Disk Usage

* Delete code and config paths that create or rely on `ATTACHMENTS_DIR` (except for legacy or tests).
* Remove any `FileResponse` usage that served local files.
* Ensure `.gitignore` no longer references `data/uploads` if unused.

---

## 13) Frontend Impact

* No code changes required if your UI already uses `attachment.deliveryUrl` / `image_url.url` as a generic URL.
* Images now load from `https://storage.googleapis.com/...` signed links.

---

## 14) Testing Checklist

* Upload ≤10MB and >10MB files (expect 413/validation for oversize).
* Verify GCS objects created under `session_id/{attachment_id}__{filename}`.
* Confirm chat history returns signed URLs and images render.
* Force-expire a URL (set `signed_url_expires_at` in DB to past) → fetch messages → see it re-signed automatically.
* Delete attachment via API (if you expose it) or wait for retention → object gone from bucket, DB row removed.

---

## 15) Nice-to-Haves (Optional)

* GCS **Lifecycle Rule** as a safety net (delete objects older than `ATTACHMENTS_RETENTION_DAYS + 2`).
* Add **Content-Disposition: inline; filename="..."** metadata when uploading, if you care about downloads naming.
* If you expect very large uploads: stream to a temp file and `upload_filelike` to avoid RAM spikes.

---

## 16) What We Explicitly Removed

* **All endpoints that download or stream images locally** (e.g., `GET /api/uploads/{id}/content`).
* **Local disk persistence** for attachments (`ATTACHMENTS_DIR` no longer used).
* **Proxy “GCS_PUBLIC_URL_TEMPLATE”** is **not used** for access; signed URLs are returned directly.

---

If you want, I can turn sections 3–9 into concrete diffs against your current files; but the above structure should be exactly what Codex needs to scaffold the changes.



_______________________________________________________________________________________________________








Here’s a clean split into two big chunks that keeps risk low and makes it easy to ship incrementally.

# Task 1 — Store to GCS and return signed URLs (Write Path)

## Goal

All new images (user uploads + model-generated) are written to **private GCS**, and the API immediately returns **signed URLs** (valid for the retention window). No local disk persistence.

## Scope (maps to your sections)

* **Dependencies & Settings**: 1, 2
* **GCS Utility**: 3
* **DB Migration**: 4
* **Blob Naming**: 5
* **Attachment Service (write path only)**: 6 (`save_user_upload`, `save_model_image_bytes`)
* **Repository updates**: 7 (insert/get/delete fields for `gcs_blob`, `signed_url`, `signed_url_expires_at`)
* **Routes – POST /api/uploads**: 8.1 (return signed URLs)

## Deliverables

* New/updated modules: `app/services/gcs.py`, `app/services/attachments.py`, `app/services/attachments_naming.py`, `app/db/attachments_repo.py`.
* Migration applied adding: `gcs_blob`, `signed_url`, `signed_url_expires_at`.
* Env wired: `GCS_BUCKET_NAME`, `GCP_PROJECT_ID`, `GOOGLE_APPLICATION_CREDENTIALS`, `ATTACHMENTS_MAX_SIZE_BYTES`, `ATTACHMENTS_RETENTION_DAYS`.
* Uploads now appear in GCS as `session_id/{attachment_id}__{filename}`.

## Acceptance Criteria

* Upload ≤10MB succeeds; >10MB is rejected.
* New rows populate `gcs_blob`, `signed_url`, `signed_url_expires_at`; UI receives `displayUrl`/`deliveryUrl` equal to the signed URL.
* Model-generated images are stored the same way and returned with signed URLs.
* No local files are created beyond ephemeral temp streams.

## Risks / Notes

* Ensure service account has `storage.objects.create`, `get`, `delete`.
* Signed URL TTL must equal retention policy to avoid dangling records.
* Backfill is **not** required; legacy attachments are handled in Task 2.

---

# Task 2 — Read-path URL refreshing, deprecations, and retention (Read Path + Ops)

## Goal

When returning chat history, URLs are always fresh (auto-resigned if expired). Local download/serve endpoints are retired. Expired attachments are cleaned from GCS + DB. Frontend keeps working with no changes.

## Scope (maps to your sections)

* **Message Serialization (refresh on read)**: 9
* **Assistant Output wiring (already uses signed URLs)**: 10 (verify only)
* **Cleanup / Retention job**: 11
* **Remove Local Disk Usage & old endpoints**: 12, 8.2, 16
* **Frontend Impact**: 13 (confirm no changes)
* **Testing checklist**: 14 (execute relevant read/refresh/cleanup tests)
* **Optional GCS lifecycle rule**: 15 (nice-to-have)

## Deliverables

* `ensure_fresh_signed_url` (or equivalent) hooked into message serialization; expired or missing `signed_url` is re-signed, persisted, and surfaced as `displayUrl`/`deliveryUrl`.
* A scheduled job (or cron) that deletes expired attachments from GCS and prunes DB rows.
* Old download/stream endpoints removed or return **410 Gone**.
* Local disk code paths and configs removed (except legacy/test scaffolding if needed).

## Acceptance Criteria

* Chat history always returns valid, loadable image URLs; manual expiry in DB triggers re-sign on next read.
* Retention job removes objects/rows older than `ATTACHMENTS_RETENTION_DAYS`.
* No references to `ATTACHMENTS_DIR`, no file-serving endpoints.
* Frontend renders images from signed GCS links without any UI change.

## Risks / Notes

* Ensure refresh logic is idempotent and tolerant of missing/legacy fields.
* Consider adding a **GCS Lifecycle Rule** as a second line of defense (objects older than `RETENTION+2d` auto-deleted).

---

## Rollout Order & Cutover

1. **Task 1** to production: New writes go to GCS with signed URLs (legacy reads still function if present).
2. **Task 2**: Enable read-path refresh, retire old endpoints, and activate the retention job.

This two-step sequencing lets you validate storage + URL signing end-to-end before touching read paths and cleanup.
